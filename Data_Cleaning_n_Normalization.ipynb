{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kjjh9ITmSAqr",
        "outputId": "e3c12214-319f-4f84-8fe8-3e56ded7c132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpofmBCsSzND",
        "outputId": "0d8fca7f-2c5e-41fe-df4a-98a5f0da6433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUDMesr6S3Vo",
        "outputId": "5503133b-99e8-4ecb-9940-726ec37318e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16WgHBRJS6aS",
        "outputId": "d3c1d669-8921-4ef5-d8f8-f57fa233b0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/232.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsR6BrJYS-er",
        "outputId": "41ac69aa-c361-42af-c78b-a2a83dc059f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAcO4v8DTBWt",
        "outputId": "20162062-48b9-4534-83cd-d01000c3417e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-08 17:18:39.103486: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-08 17:18:40.253404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install collection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjMR4MR8THFs",
        "outputId": "76351a9d-01e7-4cdf-98c1-dec172a499dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting collection\n",
            "  Downloading collection-0.1.6.tar.gz (5.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: collection\n",
            "  Building wheel for collection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for collection: filename=collection-0.1.6-py3-none-any.whl size=5099 sha256=c2d246f4cdfc881a4c693f220657f4cda16e78f053ad3402f93c8e8f30b832b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/70/eb/1d28795e9384ab3b9be6359bdde9e1652f6e7dab9d26844f70\n",
            "Successfully built collection\n",
            "Installing collected packages: collection\n",
            "Successfully installed collection-0.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pipwin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C2uC0YvTLlR",
        "outputId": "cda37b77-4bb9-4896-b8ec-62fad6e3919d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pipwin\n",
            "  Downloading pipwin-0.5.2.tar.gz (7.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from pipwin)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pipwin) (2.31.0)\n",
            "Collecting pyprind (from pipwin)\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pipwin) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from pipwin) (4.11.2)\n",
            "Collecting js2py (from pipwin)\n",
            "  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pipwin) (23.2)\n",
            "Collecting pySmartDL>=1.3.1 (from pipwin)\n",
            "  Downloading pySmartDL-1.3.4-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.9.0->pipwin) (2.5)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py->pipwin) (5.1)\n",
            "Collecting pyjsparser>=2.5.1 (from js2py->pipwin)\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pipwin) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pipwin) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pipwin) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pipwin) (2023.7.22)\n",
            "Building wheels for collected packages: pipwin, docopt, pyjsparser\n",
            "  Building wheel for pipwin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pipwin: filename=pipwin-0.5.2-py2.py3-none-any.whl size=8769 sha256=94b23bf487ff198c48f18a1583566fd078eeb871776511bd7d0527a80a0666ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/2c/53/c5a91c548b9f030b592608c24efda23ff966b1cceac6414765\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=40ab90d218ebe20bbd05d5add7a3341fc4b848d80a56df880d789b3c8befb85f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25982 sha256=3018d2650ad06693193ec74447f05ceb9c2392554fc3b400026ffe95e73a309b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
            "Successfully built pipwin docopt pyjsparser\n",
            "Installing collected packages: pySmartDL, pyprind, pyjsparser, docopt, js2py, pipwin\n",
            "Successfully installed docopt-0.6.2 js2py-0.74 pipwin-0.5.2 pySmartDL-1.3.4 pyjsparser-2.7.1 pyprind-2.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pipwin install jsonlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMp92yggTPNU",
        "outputId": "eae4e0f5-35f1-474a-c02a-dc2fe88f1b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pipwin/command.py:66: UserWarning: Found a non Windows system. Package installation might not work.\n",
            "  warn(\"Found a non Windows system. Package installation might not work.\")\n",
            "Building cache. Hang on . . .\n",
            "Done\n",
            "Package `jsonlib` not found\n",
            "Try `pipwin refresh`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim_sum_ext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc4cP4YXTQ9F",
        "outputId": "6d82f5e9-52a9-4a3a-8b58-41e62918cb20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim_sum_ext\n",
            "  Downloading gensim_sum_ext-0.1.2.tar.gz (3.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from gensim_sum_ext) (4.3.2)\n",
            "Collecting pycorenlp (from gensim_sum_ext)\n",
            "  Downloading pycorenlp-0.3.0.tar.gz (1.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim->gensim_sum_ext) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->gensim_sum_ext) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->gensim_sum_ext) (6.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pycorenlp->gensim_sum_ext) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pycorenlp->gensim_sum_ext) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pycorenlp->gensim_sum_ext) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pycorenlp->gensim_sum_ext) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pycorenlp->gensim_sum_ext) (2023.7.22)\n",
            "Building wheels for collected packages: gensim_sum_ext, pycorenlp\n",
            "  Building wheel for gensim_sum_ext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gensim_sum_ext: filename=gensim_sum_ext-0.1.2-py3-none-any.whl size=5021 sha256=e1e74ca63718fc46ccbf51a6c2e55f3276040b340ef4e6be37215914d1ec0651\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/ee/7d/a18fe7916ec3ad8e334d4eb0f9dc6034f0922910d58b29b5c0\n",
            "  Building wheel for pycorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycorenlp: filename=pycorenlp-0.3.0-py3-none-any.whl size=2120 sha256=2a1efe8def91658a9458491612b7959b480003f9cf1c85336c2891271ee4b92e\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/91/be/b83633256a1655afb34c5ea44b3290af84417a144e1f13e56f\n",
            "Successfully built gensim_sum_ext pycorenlp\n",
            "Installing collected packages: pycorenlp, gensim_sum_ext\n",
            "Successfully installed gensim_sum_ext-0.1.2 pycorenlp-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install heapq_max"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNqd-lPJTfUl",
        "outputId": "e622465f-0a09-4302-e249-2646fd95d316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting heapq_max\n",
            "  Downloading heapq_max-0.21.zip (7.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: heapq_max\n",
            "  Building wheel for heapq_max (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for heapq_max: filename=heapq_max-0.21-py3-none-any.whl size=3575 sha256=e26dbff797fb473a7a04c2e3694d829c44516f3ac72453a714cfefb1d37746f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/09/e2/785c10b67b8a9bd63bd368c6c5996b1868afe342dc3c70a571\n",
            "Successfully built heapq_max\n",
            "Installing collected packages: heapq_max\n",
            "Successfully installed heapq_max-0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install operators"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8MQJRZRTlwI",
        "outputId": "7673e4f1-1f4a-4164-f650-f265f0d4182a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting operators\n",
            "  Downloading operators-1.0.1-py3-none-any.whl (1.8 kB)\n",
            "Installing collected packages: operators\n",
            "Successfully installed operators-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python --version\n",
        "!pip install nltk\n",
        "#!pip3 install \"gensim==3.6.0\"\n",
        "#!pip install -U gensim\n",
        "!python -m pip install scipy\n",
        "!pip install numpy\n",
        "#!python -m pip install -U gensim\n",
        "!pip3 install gensim -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU6P1RaqTvND",
        "outputId": "094a3c03-1b81-488f-ea4a-19567f44904d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.3)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIgt1VZMTytG",
        "outputId": "6e01b4b6-f3e1-4cfb-bb74-32d578652d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim_sum_ext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72YRNUcyT2m7",
        "outputId": "cea34172-3ba8-4349-9bb6-685168c0a08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim_sum_ext in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from gensim_sum_ext) (4.3.2)\n",
            "Requirement already satisfied: pycorenlp in /usr/local/lib/python3.10/dist-packages (from gensim_sum_ext) (0.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim->gensim_sum_ext) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->gensim_sum_ext) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->gensim_sum_ext) (6.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pycorenlp->gensim_sum_ext) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pycorenlp->gensim_sum_ext) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pycorenlp->gensim_sum_ext) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pycorenlp->gensim_sum_ext) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pycorenlp->gensim_sum_ext) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL8jayyET8Hs",
        "outputId": "72fcbd77-ed91-4ecd-e1b0-0d6c295be7ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from collections import Counter\n",
        "import json\n",
        "from spacy.lang.en import English\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "import PyPDF2\n",
        "import re\n",
        "import nltk\n",
        "import math\n",
        "import gensim_sum_ext\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "        text = \"\"\n",
        "        for page in range(num_pages):\n",
        "            text += pdf_reader.pages[page].extract_text()\n",
        "    return text\n",
        "\n",
        "def ner_and_keywords(directory):\n",
        "    texts = []\n",
        "    doc_counts = Counter()\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.pdf'):\n",
        "            pdf_path = os.path.join(directory, filename)\n",
        "            text = extract_text_from_pdf(pdf_path)\n",
        "            doc = nlp(text)\n",
        "            tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
        "            texts.append(tokens)\n",
        "            doc_counts.update(tokens)\n",
        "    return texts, doc_counts\n",
        "\n",
        "# Function for Spacy Summarization\n",
        "def text_summarizer(text, percentage):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    def summarize_single_text(single_text):\n",
        "        doc = nlp(single_text)\n",
        "\n",
        "        freq_of_word = {}\n",
        "        for word in doc:\n",
        "            if word.text.lower() not in nlp.Defaults.stop_words and word.text.lower() not in punctuation:\n",
        "                if word.text not in freq_of_word:\n",
        "                    freq_of_word[word.text] = 1\n",
        "                else:\n",
        "                    freq_of_word[word.text] += 1\n",
        "\n",
        "        max_freq = max(freq_of_word.values())\n",
        "\n",
        "        for word in freq_of_word:\n",
        "            freq_of_word[word] = freq_of_word[word] / max_freq\n",
        "\n",
        "        sent_scores = {}\n",
        "        for sent in doc.sents:\n",
        "            for word in sent:\n",
        "                if word.text.lower() in freq_of_word:\n",
        "                    if sent not in sent_scores:\n",
        "                        sent_scores[sent] = freq_of_word[word.text.lower()]\n",
        "                    else:\n",
        "                        sent_scores[sent] += freq_of_word[word.text.lower()]\n",
        "\n",
        "        len_tokens = int(int(sum(1 for _ in doc.sents)) * percentage)\n",
        "\n",
        "        summary = nlargest(n=len_tokens, iterable=sent_scores, key=sent_scores.get)\n",
        "        final_summary = [word.text for sent in summary for word in sent]\n",
        "\n",
        "        return ' '.join(final_summary)\n",
        "\n",
        "    summary = summarize_single_text(text)\n",
        "    return summary\n",
        "\n",
        "# Function for LDA Topic Modeling\n",
        "def create_lda_model(texts, num_topics=10, num_words=10):\n",
        "    dictionary = corpora.Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
        "    return lda_model\n",
        "'''\n",
        "def write_to_json(data, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f)\n",
        "'''\n",
        "def get_top_topics(lda_model, num_topics=10):\n",
        "    top_topics = []\n",
        "    for idx, topic in lda_model.print_topics(-1):\n",
        "        top_topics.append(idx)\n",
        "        if len(top_topics) == num_topics:\n",
        "            break\n",
        "    return top_topics\n",
        "\n",
        "def get_top_topic(lda_model, doc_counts):\n",
        "    top_topic = None\n",
        "    max_count = 0\n",
        "    for topic_idx, topic in lda_model.print_topics(-1):\n",
        "        topic_count = sum([doc_counts[word] for word in topic.split(' ')])\n",
        "        if topic_count > max_count:\n",
        "            max_count = topic_count\n",
        "            top_topic = topic_idx\n",
        "    return top_topic\n",
        "\n",
        "# Function to extract topics and summaries\n",
        "def extract_topics_and_summaries(text):\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\b[a-z]+\\b', ' ', text)\n",
        "    text = text.split()\n",
        "    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]\n",
        "    text = ' '.join(text)\n",
        "    topics = []\n",
        "    summaries = []\n",
        "    for i in range(0, len(text), 100):\n",
        "        topic = text[i:i+100]\n",
        "        topics.append(topic)\n",
        "        summary = text_summarizer([topic], 0.2)[0]  # Fixing the summarize call\n",
        "        summaries.append(summary)\n",
        "    return topics, summaries\n",
        "\n",
        "def main():\n",
        "    directory = '/content/Content'\n",
        "    output_file = 'doc_counts.json'\n",
        "\n",
        "    # NER and Keyword Extraction\n",
        "    texts, doc_counts = ner_and_keywords(directory)\n",
        "    write_to_json(dict(doc_counts), output_file)\n",
        "\n",
        "    # Spacy Summarization\n",
        "    text_to_summarize = \"Your input text here\"  # Replace with your actual input text\n",
        "    final_summary = text_summarizer(text_to_summarize, 0.2)\n",
        "    print(\"Spacy Summarization:\")\n",
        "    print(final_summary)\n",
        "\n",
        "    # LDA Topic Modeling\n",
        "    lda_model = create_lda_model(texts)\n",
        "    print(\"LDA Model Created\")\n",
        "    for idx, topic in lda_model.print_topics(-1):\n",
        "        print(f\"Topic {idx + 1} : {topic}\")\n",
        "\n",
        "    # Get top topics\n",
        "    top_topics = get_top_topics(lda_model)\n",
        "    print(\"Top Topics:\")\n",
        "    for idx, topic_idx in enumerate(top_topics):\n",
        "        print(f\"Top Topic {idx + 1}: Topic {topic_idx}\")\n",
        "\n",
        "    # Get top topic\n",
        "    top_topic = get_top_topic(lda_model, doc_counts)\n",
        "    print(f\"Top Topic: {top_topic}\")\n",
        "\n",
        "    # Code2: Extract topics and summaries from PDF\n",
        "    pdf_path = '/content/Content/Doc_1.pdf'\n",
        "    text_from_pdf = extract_text_from_pdf(pdf_path)\n",
        "    topics_from_pdf, summaries_from_pdf = extract_topics_and_summaries(text_from_pdf)\n",
        "    for i in range(len(topics_from_pdf)):\n",
        "        print(f\"Topic {i + 1}: {topics_from_pdf[i]}\")\n",
        "        print(f\"Summary {i + 1}: {summaries_from_pdf[i]}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnOBuevMT-bF",
        "outputId": "18a6df50-a711-40bc-dece-585d7e138536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spacy Summarization:\n",
            "\n",
            "LDA Model Created\n",
            "Topic 1 : 1.000*\"\"\n",
            "Topic 2 : 1.000*\"\"\n",
            "Topic 3 : 1.000*\"\"\n",
            "Topic 4 : 1.000*\"\"\n",
            "Topic 5 : 1.000*\"\"\n",
            "Topic 6 : 1.000*\"\"\n",
            "Topic 7 : 1.000*\"\"\n",
            "Topic 8 : 1.000*\"\"\n",
            "Topic 9 : 1.000*\"\"\n",
            "Topic 10 : 1.000*\"\"\n",
            "Top Topics:\n",
            "Top Topic 1: Topic 0\n",
            "Top Topic 2: Topic 1\n",
            "Top Topic 3: Topic 2\n",
            "Top Topic 4: Topic 3\n",
            "Top Topic 5: Topic 4\n",
            "Top Topic 6: Topic 5\n",
            "Top Topic 7: Topic 6\n",
            "Top Topic 8: Topic 7\n",
            "Top Topic 9: Topic 8\n",
            "Top Topic 10: Topic 9\n",
            "Top Topic: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfx"
      ],
      "metadata": {
        "id": "Mtuqrpq2aBZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a43c11-268a-4dcc-a259-0b602f4c4cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfx in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: pdfminer.six==20201018 in /usr/local/lib/python3.10/dist-packages (from pdfx) (20201018)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfx) (4.0.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20201018->pdfx) (41.0.4)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20201018->pdfx) (2.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->pdfminer.six==20201018->pdfx) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20201018->pdfx) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from collections import Counter\n",
        "import json\n",
        "from spacy.lang.en import English\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "import pdfx\n",
        "import PyPDF2\n",
        "import re\n",
        "import nltk\n",
        "import math\n",
        "import gensim_sum_ext\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import pdfx\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_text_from_pdf():\n",
        "    pdf = pdfx.PDFx(\"https://nodis3.gsfc.nasa.gov/OPD_docs/NID_8715-129_.pdf\")\n",
        "    text = pdf.get_text()\n",
        "\n",
        "    return text\n",
        "\n",
        "def ner_and_keywords(text):\n",
        "    texts = []\n",
        "    doc_counts = Counter()\n",
        "    for filename in os.listdir(text):\n",
        "            doc = nlp(text)\n",
        "            tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
        "            texts.append(tokens)\n",
        "            doc_counts.update(tokens)\n",
        "    return texts, doc_counts\n",
        "\n",
        "# Function for Spacy Summarization\n",
        "def text_summarizer(text, percentage):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    def summarize_single_text(single_text):\n",
        "        doc = nlp(single_text)\n",
        "\n",
        "        freq_of_word = {}\n",
        "        for word in doc:\n",
        "            if word.text.lower() not in nlp.Defaults.stop_words and word.text.lower() not in punctuation:\n",
        "                if word.text not in freq_of_word:\n",
        "                    freq_of_word[word.text] = 1\n",
        "                else:\n",
        "                    freq_of_word[word.text] += 1\n",
        "\n",
        "        max_freq = max(freq_of_word.values())\n",
        "\n",
        "        for word in freq_of_word:\n",
        "            freq_of_word[word] = freq_of_word[word] / max_freq\n",
        "\n",
        "        sent_scores = {}\n",
        "        for sent in doc.sents:\n",
        "            for word in sent:\n",
        "                if word.text.lower() in freq_of_word:\n",
        "                    if sent not in sent_scores:\n",
        "                        sent_scores[sent] = freq_of_word[word.text.lower()]\n",
        "                    else:\n",
        "                        sent_scores[sent] += freq_of_word[word.text.lower()]\n",
        "\n",
        "        len_tokens = int(int(sum(1 for _ in doc.sents)) * percentage)\n",
        "\n",
        "        summary = nlargest(n=len_tokens, iterable=sent_scores, key=sent_scores.get)\n",
        "        final_summary = [word.text for sent in summary for word in sent]\n",
        "\n",
        "        return ' '.join(final_summary)\n",
        "\n",
        "    summary = summarize_single_text(text)\n",
        "    return summary\n",
        "\n",
        "# Function for LDA Topic Modeling\n",
        "def create_lda_model(texts, num_topics=10, num_words=10):\n",
        "    dictionary = corpora.Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
        "    return lda_model\n",
        "\n",
        "def write_to_json(data, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f)\n",
        "\n",
        "def get_top_topics(lda_model, num_topics=10):\n",
        "    top_topics = []\n",
        "    for idx, topic in lda_model.print_topics(-1):\n",
        "        top_topics.append(idx)\n",
        "        if len(top_topics) == num_topics:\n",
        "            break\n",
        "    return top_topics\n",
        "\n",
        "def get_top_topic(lda_model, doc_counts):\n",
        "    top_topic = None\n",
        "    max_count = 0\n",
        "    for topic_idx, topic in lda_model.print_topics(-1):\n",
        "        topic_count = sum([doc_counts[word] for word in topic.split(' ')])\n",
        "        if topic_count > max_count:\n",
        "            max_count = topic_count\n",
        "            top_topic = topic_idx\n",
        "    return top_topic\n",
        "\n",
        "# Function to extract topics and summaries\n",
        "def extract_topics_and_summaries(text):\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\b[a-z]+\\b', ' ', text)\n",
        "    text = text.split()\n",
        "    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]\n",
        "    text = ' '.join(text)\n",
        "    topics = []\n",
        "    summaries = []\n",
        "    for i in range(0, len(text), 100):\n",
        "        topic = text[i:i+100]\n",
        "        topics.append(topic)\n",
        "        summary = text_summarizer([topic], 0.2)[0]  # Fixing the summarize call\n",
        "        summaries.append(summary)\n",
        "    return topics, summaries\n",
        "\n",
        "def main():\n",
        "    directory = '/content/Content'\n",
        "    output_file = 'doc_counts.json'\n",
        "\n",
        "    # NER and Keyword Extraction\n",
        "    texts, doc_counts = ner_and_keywords(directory)\n",
        "    write_to_json(dict(doc_counts), output_file)\n",
        "\n",
        "    # Spacy Summarization\n",
        "    text_to_summarize = \"Your input text here\"  # Replace with your actual input text\n",
        "    final_summary = text_summarizer(text_to_summarize, 0.2)\n",
        "    print(\"Spacy Summarization:\")\n",
        "    print(final_summary)\n",
        "\n",
        "    # LDA Topic Modeling\n",
        "    lda_model = create_lda_model(texts)\n",
        "    print(\"LDA Model Created\")\n",
        "    for idx, topic in lda_model.print_topics(-1):\n",
        "        print(f\"Topic {idx + 1} : {topic}\")\n",
        "\n",
        "    # Get top topics\n",
        "    top_topics = get_top_topics(lda_model)\n",
        "    print(\"Top Topics:\")\n",
        "    for idx, topic_idx in enumerate(top_topics):\n",
        "        print(f\"Top Topic {idx + 1}: Topic {topic_idx}\")\n",
        "\n",
        "    # Get top topic\n",
        "    top_topic = get_top_topic(lda_model, doc_counts)\n",
        "    print(f\"Top Topic: {top_topic}\")\n",
        "\n",
        "    # Code2: Extract topics and summaries from PDF\n",
        "    pdf_path = '/content/Content/Doc_1.pdf'\n",
        "    text_from_pdf = extract_text_from_pdf(pdf_path)\n",
        "    topics_from_pdf, summaries_from_pdf = extract_topics_and_summaries(text_from_pdf)\n",
        "    for i in range(len(topics_from_pdf)):\n",
        "        print(f\"Topic {i + 1}: {topics_from_pdf[i]}\")\n",
        "        print(f\"Summary {i + 1}: {summaries_from_pdf[i]}\")\n",
        "\"\"\"\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "SgZCu5hsidUM",
        "outputId": "34384993-1051-4286-8c32-8e5946cc3892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Function for Spacy Summarization\\ndef text_summarizer(text, percentage):\\n    nlp = spacy.load(\\'en_core_web_sm\\')\\n\\n    def summarize_single_text(single_text):\\n        doc = nlp(single_text)\\n\\n        freq_of_word = {}\\n        for word in doc:\\n            if word.text.lower() not in nlp.Defaults.stop_words and word.text.lower() not in punctuation:\\n                if word.text not in freq_of_word:\\n                    freq_of_word[word.text] = 1\\n                else:\\n                    freq_of_word[word.text] += 1\\n\\n        max_freq = max(freq_of_word.values())\\n\\n        for word in freq_of_word:\\n            freq_of_word[word] = freq_of_word[word] / max_freq\\n\\n        sent_scores = {}\\n        for sent in doc.sents:\\n            for word in sent:\\n                if word.text.lower() in freq_of_word:\\n                    if sent not in sent_scores:\\n                        sent_scores[sent] = freq_of_word[word.text.lower()]\\n                    else:\\n                        sent_scores[sent] += freq_of_word[word.text.lower()]\\n\\n        len_tokens = int(int(sum(1 for _ in doc.sents)) * percentage)\\n\\n        summary = nlargest(n=len_tokens, iterable=sent_scores, key=sent_scores.get)\\n        final_summary = [word.text for sent in summary for word in sent]\\n\\n        return \\' \\'.join(final_summary)\\n\\n    summary = summarize_single_text(text)\\n    return summary\\n\\n# Function for LDA Topic Modeling\\ndef create_lda_model(texts, num_topics=10, num_words=10):\\n    dictionary = corpora.Dictionary(texts)\\n    corpus = [dictionary.doc2bow(text) for text in texts]\\n    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\\n    return lda_model\\n\\ndef write_to_json(data, filename):\\n    with open(filename, \\'w\\') as f:\\n        json.dump(data, f)\\n\\ndef get_top_topics(lda_model, num_topics=10):\\n    top_topics = []\\n    for idx, topic in lda_model.print_topics(-1):\\n        top_topics.append(idx)\\n        if len(top_topics) == num_topics:\\n            break\\n    return top_topics\\n\\ndef get_top_topic(lda_model, doc_counts):\\n    top_topic = None\\n    max_count = 0\\n    for topic_idx, topic in lda_model.print_topics(-1):\\n        topic_count = sum([doc_counts[word] for word in topic.split(\\' \\')])\\n        if topic_count > max_count:\\n            max_count = topic_count\\n            top_topic = topic_idx\\n    return top_topic\\n\\n# Function to extract topics and summaries\\ndef extract_topics_and_summaries(text):\\n    nltk.download(\\'punkt\\')\\n    nltk.download(\\'stopwords\\')\\n    stop_words = set(nltk.corpus.stopwords.words(\\'english\\'))\\n    lemmatizer = nltk.stem.WordNetLemmatizer()\\n    text = re.sub(\\'[^a-zA-Z]\\', \\' \\', text)\\n    text = text.lower()\\n    text = re.sub(r\\'\\x08[a-z]+\\x08\\', \\' \\', text)\\n    text = text.split()\\n    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]\\n    text = \\' \\'.join(text)\\n    topics = []\\n    summaries = []\\n    for i in range(0, len(text), 100):\\n        topic = text[i:i+100]\\n        topics.append(topic)\\n        summary = text_summarizer([topic], 0.2)[0]  # Fixing the summarize call\\n        summaries.append(summary)\\n    return topics, summaries\\n\\ndef main():\\n    directory = \\'/content/Content\\'\\n    output_file = \\'doc_counts.json\\'\\n\\n    # NER and Keyword Extraction\\n    texts, doc_counts = ner_and_keywords(directory)\\n    write_to_json(dict(doc_counts), output_file)\\n\\n    # Spacy Summarization\\n    text_to_summarize = \"Your input text here\"  # Replace with your actual input text\\n    final_summary = text_summarizer(text_to_summarize, 0.2)\\n    print(\"Spacy Summarization:\")\\n    print(final_summary)\\n\\n    # LDA Topic Modeling\\n    lda_model = create_lda_model(texts)\\n    print(\"LDA Model Created\")\\n    for idx, topic in lda_model.print_topics(-1):\\n        print(f\"Topic {idx + 1} : {topic}\")\\n\\n    # Get top topics\\n    top_topics = get_top_topics(lda_model)\\n    print(\"Top Topics:\")\\n    for idx, topic_idx in enumerate(top_topics):\\n        print(f\"Top Topic {idx + 1}: Topic {topic_idx}\")\\n\\n    # Get top topic\\n    top_topic = get_top_topic(lda_model, doc_counts)\\n    print(f\"Top Topic: {top_topic}\")\\n\\n    # Code2: Extract topics and summaries from PDF\\n    pdf_path = \\'/content/Content/Doc_1.pdf\\'\\n    text_from_pdf = extract_text_from_pdf(pdf_path)\\n    topics_from_pdf, summaries_from_pdf = extract_topics_and_summaries(text_from_pdf)\\n    for i in range(len(topics_from_pdf)):\\n        print(f\"Topic {i + 1}: {topics_from_pdf[i]}\")\\n        print(f\"Summary {i + 1}: {summaries_from_pdf[i]}\")\\n\"\"\"\\nif __name__ == \\'__main__\\':\\n    main()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LAGpca4J6nEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tOYHp_pRyB6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A9tp_OOalUR2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}