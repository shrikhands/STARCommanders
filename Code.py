# -*- coding: utf-8 -*-
"""SpaceApp: Star Commanders

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rZA2mGbI-Dc5NV7Oc5jwwLkE3hTIo1f3
"""

!pip install spacy

!pip install gensim

!pip install -U scikit-learn

!pip install PyPDF2

!pip install nltk

!python -m spacy download en_core_web_sm

!pip install collection

!pip install pipwin

!pipwin install jsonlib

!pipwin refresh

!pip install operators

!pip install heapq_max

!pip install gensim_sum_ext

! python --version
!pip install nltk
#!pip3 install "gensim==3.6.0"
#!pip install -U gensim
!python -m pip install scipy
!pip install numpy
#!python -m pip install -U gensim
!pip3 install gensim -U

!pip install PyPDF2

!pip install gensim_sum_ext

!pip install lxml

import os
import spacy
import gensim
from gensim import corpora
from collections import Counter
import json
import operator
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation
from heapq import nlargest
import PyPDF2
import re
import nltk


import gensim_sum_ext
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Function for NER and Keyword Extraction
def ner_and_keywords(directory):
    nlp = spacy.load('en_core_web_sm')
    texts = []
    doc_counts = Counter()
    for filename in os.listdir(directory):
        if filename.endswith('.pdf'):
            with open(os.path.join(directory, filename), 'rb') as file:
                text = file.read()
                doc = nlp(text).decode("utf-8")
                texts.append([token.lemma_ for token in doc if token.is_alpha and not token.is_stop])
                doc_counts.update([token.lemma_ for token in doc if token.is_alpha and not token.is_stop])
    return texts, doc_counts

# Function for Spacy Summarization
def text_summarizer(texts, percentage):
    nlp = spacy.load('en_core_web_sm')

    def summarize_single_text(single_text):
        doc = nlp(single_text)

        freq_of_word = {}
        for word in doc:
            if word.text.lower() not in nlp.Defaults.stop_words and word.text.lower() not in punctuation:
                if word.text not in freq_of_word:
                    freq_of_word[word.text] = 1
                else:
                    freq_of_word[word.text] += 1

        max_freq = max(freq_of_word.values())

        for word in freq_of_word:
            freq_of_word[word] = freq_of_word[word] / max

        sent_scores = {}
        for sent in doc.sents:
            for word in sent:
                if word.text.lower() in freq_of_word:
                    if sent not in sent_scores:
                        sent_scores[sent] = freq_of_word[word.text.lower()]
                    else:
                        sent_scores[sent] += freq_of_word[word.text.lower()]

        len_tokens = int(sum(1 for _ in doc.sents)) * percentage

        summary = nlargest(n=len_tokens, iterable=sent_scores, key=sent_scores.get)
        final_summary = [word.text for sent in summary for word in sent]

        return ' '.join(final_summary)

    summaries = [summarize_single_text(text) for text in texts]
    return summaries

# Function for LDF Topic Modeling
def create_lda_model(texts, num_topics=10, num_words=10):
    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)
    return lda_model

def write_to_json(data, filename):
    with open(filename, 'w') as f:
        json.dump(data, f)

def get_top_topics(lda_model, num_topics=10):
    top_topics = []
    for idx, topic in lda_model.print_topics(-1):
        top_topics.append(idx)
        if len(top_topics) == num_topics:
            break
    return top_topics

def get_top_topic(lda_model, doc_counts):
    top_topic = None
    max_count = 0
    for topic_idx, topic in lda_model.print_topics(-1):
        topic_count = sum([doc_counts[word] for word in topic[1].split(' ')])
        if topic_count > max_count:
            max_count = topic_count
            top_topic = topic_idx
    return top_topic

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    with open(pdf_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfFileReader(file)
        num_pages = pdf_reader.numPages
        text = ""
        for page in range(num_pages):
            text += pdf_reader.getPage(page).extractText()
    return text

# Function to extract topics and summaries
def extract_topics_and_summaries(text):
    nltk.download('punkt')
    nltk.download('stopwords')
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    text = re.sub('[^a-zA-Z]', ' ', text)
    text = text.lower()
    text = re.sub(r'\b[a-z]+ \b\w*', ' ', text)
    text = text.split()
    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]
    text = ' '.join(text)
    topics = []
    summaries = []
    for i in range(0, len(text), 100):
        topic = text[i:i+100]
        topics.append(topic)
        summary = summarize(text[i:i+100])
        summaries.append(summary)
    return topics, summaries

def main():
    directory = '/content/Content'
    output_file = 'doc_counts.json'

    # NER and Keyword Extraction
    texts, doc_counts = ner_and_keywords(directory)
    write_to_json(doc_counts, output_file)

    # Spacy Summarization
    text_to_summarize = "Your input text here"  # Replace with your actual input text
    final_summary = text_summarizer(text_to_summarize, 0.2)

    # LDF Topic Modeling
    lda_model = create_lda_model(texts)
    print("LDA Model Created")
    for idx, topic in lda_model.print_topics(-1):
        print(f"Topic {idx+1} : {topic}")

    # Get top topics
    top_topics = get_top_topics(lda_model)
    for idx, topic_idx in enumerate(top_topics):
        print(f"Top Topic {idx+1}: Topic {topic_idx}")

    # Get top topic
    top_topic = get_top_topic(lda_model, doc_counts)
    print(f"Top Topic: {top_topic}")

    # Code2: Extract topics and summaries from PDF
    pdf_path = '/content/Content/Doc1.pdf'
    text_from_pdf = extract_text_from_pdf(pdf_path)
    topics_from_pdf, summaries_from_pdf = extract_topics_and_summaries(text_from_pdf)
    for i in range(len(topics_from_pdf)):
        print(f"Topic {i+1}: {topics_from_pdf[i]}")
        print(f"Summary {i+1}: {summaries_from_pdf[i]}")

if __name__ == '__main__':
    main()